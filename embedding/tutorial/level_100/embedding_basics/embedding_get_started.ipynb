{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7766076c-5b53-4a72-a0e9-d16dbac92fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 'regex' instead of 're' for \\p{C} support\n",
    "# Generally regex is better alternative to the standard re.\n",
    "import os\n",
    "import sys\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoConfig,\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26f60cdf-25f6-459e-9e2e-4d8d7bbc0c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_LIB: str = os.path.abspath(\"../../../../lib/code/python\")\n",
    "if PATH_TO_LIB not in sys.path:\n",
    "    sys.path.append(PATH_TO_LIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e832273d-817e-40ac-bb51-e0e139593fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PHTHONPATH'] = PATH_TO_LIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "676b5844-ce2d-46ab-8715-7596ceef504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from lib.util_llm.preprocessing import (\n",
    "    ChunkConfig,\n",
    "    chunk_text,\n",
    "    chunk_texts_batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b865e8e-d5c3-4ca1-a4bc-9803fc61c0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9e5add-7d33-4350-8fba-4eac707ad868",
   "metadata": {},
   "source": [
    "# Embedding Model (snowflake-arctic-embedding-s)\n",
    "\n",
    "> The snowflake-arctic-embedding models achieve state-of-the-art performance on the MTEB/BEIR leaderboard for each of their size variants.\n",
    "\n",
    "* [snowflake-arctic-embed-s](https://huggingface.co/Snowflake/snowflake-arctic-embed-s/blob/main/config.json)\n",
    "```\n",
    "{\n",
    "  \"_name_or_path\": \"/data/.model_and_tokenizer_cache/86c70943a1386ead2399854a9324005efb9328b6a9a50b66353fe62386fd6257\",\n",
    "  \"architectures\": [\n",
    "    \"BertModel\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"classifier_dropout\": null,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 384,              # <---\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 1536,\n",
    "  \"layer_norm_eps\": 1e-12,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"model_type\": \"bert\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"pad_token_id\": 0,\n",
    "  \"position_embedding_type\": \"absolute\",\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.36.1\",\n",
    "  \"type_vocab_size\": 2,\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 30522\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3ab1738-dab7-4518-82f3-f52451209ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL: str = \"Snowflake/snowflake-arctic-embed-s\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b84b26c-dbd5-4a7f-a482-5115fc1281b4",
   "metadata": {},
   "source": [
    "## Max Chunk Size (Max number of tokens)\n",
    "\n",
    "When the both fields match, then that is the context size.\n",
    "\n",
    "| Field                          | Meaning                                              |\n",
    "|--------------------------------|------------------------------------------------------|\n",
    "| tokenizer.model_max_length     | How many tokens the tokenizer will allow             |\n",
    "| config.max_position_embeddings | The max positions of tokens the model can represent. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31bb3b5d-0590-496b-b82b-bd24f483d52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer max length:512\n",
      "Config max position embeddings: 512\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n",
    "config = AutoConfig.from_pretrained(EMBEDDING_MODEL)\n",
    "\n",
    "print(f\"Tokenizer max length:{tokenizer.model_max_length}\")\n",
    "print(f\"Config max position embeddings:\", config.max_position_embeddings)\n",
    "\n",
    "if tokenizer.model_max_length == tokenizer.model_max_length:\n",
    "    CONTEXT_WINDOW_SIZE: int = tokenizer.model_max_length\n",
    "else:\n",
    "    CONTEXT_WINDOW_SIZE: int = min(\n",
    "        tokenizer.model_max_length,\n",
    "        config.max_position_embeddings\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5157b9-da80-4c36-a94b-9dc4d47467f1",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "183e38fb-350b-4e0c-ac5b-bc45e936cb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Snowflake Arctic Embed model...\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 1: Basic Chunking\n",
      "======================================================================\n",
      "\n",
      "Document split into 1 chunks\n",
      "\n",
      "Chunk sizes:\n",
      "  Chunk 1: 181 tokens, 1070 characters\n",
      "\n",
      "First chunk preview:\n",
      "  machine learning is a subset of artificial intelligence that focuses on developing systems that can learn from and make decisions based on data. unlik...\n",
      "\n",
      "Generating embeddings for 1 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1000 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (1, 384)\n",
      "Embedding dimension: 384\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 2: Custom Configuration\n",
      "======================================================================\n",
      "\n",
      "With smaller chunks: 1 chunks\n",
      "  Chunk 1: 181 tokens\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 3: Processing Multiple Documents\n",
      "======================================================================\n",
      "Document 1: 1 chunks\n",
      "Document 2: 1 chunks\n",
      "Document 3: 1 chunks\n",
      "\n",
      "Total chunks across all documents: 3\n",
      "Generated 3 embeddings\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 4: Integrity Validation in Action\n",
      "======================================================================\n",
      "✓ Integrity check passed for 3 chunks\n",
      "\n",
      "Empty text produces: []\n",
      "\n",
      "======================================================================\n",
      "Tutorial complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example usage and tutorial\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    Tutorial: Using chunking with Snowflake Arctic Embed\n",
    "    \n",
    "    This example shows how to chunk long documents and embed them using\n",
    "    the Snowflake Arctic Embed sentence transformer model.\n",
    "    \"\"\"\n",
    "    print(\"Loading Snowflake Arctic Embed model...\")\n",
    "    model = SentenceTransformer('Snowflake/snowflake-arctic-embed-s')\n",
    "    \n",
    "    # Example long document (in practice, this would be much longer)\n",
    "    long_document = \"\"\"\n",
    "    Machine learning is a subset of artificial intelligence that focuses on\n",
    "    developing systems that can learn from and make decisions based on data.\n",
    "    Unlike traditional programming where rules are explicitly coded, machine\n",
    "    learning algorithms build models based on sample data, known as training data.\n",
    "    \n",
    "    The field of machine learning is closely related to computational statistics,\n",
    "    which focuses on making predictions using computers. It has strong ties to\n",
    "    mathematical optimization, which delivers methods, theory and application\n",
    "    domains to the field.\n",
    "    \n",
    "    Deep learning, a subset of machine learning, uses neural networks with\n",
    "    multiple layers to progressively extract higher-level features from raw input.\n",
    "    For example, in image processing, lower layers may identify edges, while\n",
    "    higher layers may identify concepts relevant to a human such as digits,\n",
    "    letters, or faces.\n",
    "    \n",
    "    Modern machine learning has many applications including computer vision,\n",
    "    speech recognition, email filtering, agriculture, and medicine. When applied\n",
    "    to business problems, it is known as predictive analytics.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 1: Basic Chunking\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Configure chunking for Arctic Embed\n",
    "    # Arctic Embed has a 512 token context window\n",
    "    config = ChunkConfig(\n",
    "        max_tokens=512,          # Model's maximum context length\n",
    "        overlap_tokens=50,       # Overlap for context continuity\n",
    "        validate_integrity=True  # Always validate! (default)\n",
    "    )\n",
    "    \n",
    "    # Chunk the document\n",
    "    chunks = chunk_text(long_document, model.tokenizer, config)\n",
    "    \n",
    "    print(f\"\\nDocument split into {len(chunks)} chunks\")\n",
    "    print(f\"\\nChunk sizes:\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        tokens = model.tokenizer.encode(chunk, add_special_tokens=False)\n",
    "        print(f\"  Chunk {i+1}: {len(tokens)} tokens, {len(chunk)} characters\")\n",
    "    \n",
    "    print(f\"\\nFirst chunk preview:\")\n",
    "    print(f\"  {chunks[0][:150]}...\")\n",
    "    \n",
    "    # Embed all chunks\n",
    "    print(f\"\\nGenerating embeddings for {len(chunks)} chunks...\")\n",
    "    embeddings = model.encode(chunks)\n",
    "    \n",
    "    print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "    print(f\"Embedding dimension: {embeddings[0].shape[0]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 2: Custom Configuration\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Smaller chunks with more overlap for fine-grained search\n",
    "    small_config = ChunkConfig(\n",
    "        max_tokens=256,      # Smaller chunks\n",
    "        overlap_tokens=100,  # More overlap for better context\n",
    "        validate_integrity=True\n",
    "    )\n",
    "    \n",
    "    small_chunks = chunk_text(long_document, model.tokenizer, small_config)\n",
    "    \n",
    "    print(f\"\\nWith smaller chunks: {len(small_chunks)} chunks\")\n",
    "    for i, chunk in enumerate(small_chunks):\n",
    "        tokens = model.tokenizer.encode(chunk, add_special_tokens=False)\n",
    "        print(f\"  Chunk {i+1}: {len(tokens)} tokens\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 3: Processing Multiple Documents\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    documents = [\n",
    "        \"First document about AI and machine learning...\",\n",
    "        \"Second document about deep learning architectures...\",\n",
    "        \"Third document about natural language processing...\"\n",
    "    ]\n",
    "    \n",
    "    all_chunks = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        doc_chunks = chunk_text(doc, model.tokenizer, config)\n",
    "        print(f\"Document {i+1}: {len(doc_chunks)} chunks\")\n",
    "        all_chunks.extend(doc_chunks)\n",
    "    \n",
    "    print(f\"\\nTotal chunks across all documents: {len(all_chunks)}\")\n",
    "    \n",
    "    # Embed all chunks\n",
    "    all_embeddings = model.encode(all_chunks)\n",
    "    print(f\"Generated {len(all_embeddings)} embeddings\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 4: Integrity Validation in Action\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # This will succeed\n",
    "    try:\n",
    "        test_text = \"The quick brown fox jumps over the lazy dog. \" * 100\n",
    "        test_chunks = chunk_text(test_text, model.tokenizer, config)\n",
    "        print(f\"✓ Integrity check passed for {len(test_chunks)} chunks\")\n",
    "    except ValueError as e:\n",
    "        print(f\"✗ Integrity check failed: {e}\")\n",
    "    \n",
    "    # Example: What happens with empty text\n",
    "    empty_chunks = chunk_text(\"\", model.tokenizer, config)\n",
    "    print(f\"\\nEmpty text produces: {empty_chunks}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Tutorial complete!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a4fd2b-9a93-4c33-9dff-2a2d41d604aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78bf9c2-eaae-427a-88f5-2c567db66d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad6b62-2722-429a-a57f-9a54d10429f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
