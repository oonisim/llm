{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7766076c-5b53-4a72-a0e9-d16dbac92fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from langchain_text_splitters import (\n",
    "    SentenceTransformersTokenTextSplitter,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoConfig,\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b865e8e-d5c3-4ca1-a4bc-9803fc61c0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9e5add-7d33-4350-8fba-4eac707ad868",
   "metadata": {},
   "source": [
    "# Embedding Model (snowflake-arctic-embedding-s)\n",
    "\n",
    "> The snowflake-arctic-embedding models achieve state-of-the-art performance on the MTEB/BEIR leaderboard for each of their size variants.\n",
    "\n",
    "\n",
    "\n",
    "* [snowflake-arctic-embed-s](https://huggingface.co/Snowflake/snowflake-arctic-embed-s/blob/main/config.json)\n",
    "```\n",
    "{\n",
    "  \"_name_or_path\": \"/data/.model_and_tokenizer_cache/86c70943a1386ead2399854a9324005efb9328b6a9a50b66353fe62386fd6257\",\n",
    "  \"architectures\": [\n",
    "    \"BertModel\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"classifier_dropout\": null,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 384,              # <---\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 1536,\n",
    "  \"layer_norm_eps\": 1e-12,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"model_type\": \"bert\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"pad_token_id\": 0,\n",
    "  \"position_embedding_type\": \"absolute\",\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.36.1\",\n",
    "  \"type_vocab_size\": 2,\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 30522\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3ab1738-dab7-4518-82f3-f52451209ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL: str = \"Snowflake/snowflake-arctic-embed-s\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b84b26c-dbd5-4a7f-a482-5115fc1281b4",
   "metadata": {},
   "source": [
    "## Max Chunk Size (Max number of tokens)\n",
    "\n",
    "When the both fields match, then that is the context size.\n",
    "\n",
    "| Field                          | Meaning                                              |\n",
    "|--------------------------------|------------------------------------------------------|\n",
    "| tokenizer.model_max_length     | How many tokens the tokenizer will allow             |\n",
    "| config.max_position_embeddings | The max positions of tokens the model can represent. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31bb3b5d-0590-496b-b82b-bd24f483d52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer max length:512\n",
      "Config max position embeddings: 512\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n",
    "config = AutoConfig.from_pretrained(EMBEDDING_MODEL)\n",
    "\n",
    "print(f\"Tokenizer max length:{tokenizer.model_max_length}\")\n",
    "print(f\"Config max position embeddings:\", config.max_position_embeddings)\n",
    "\n",
    "if tokenizer.model_max_length == tokenizer.model_max_length:\n",
    "    CONTEXT_WINDOW_SIZE: int = tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e6adc1a-08e5-4d7a-b0be-b64d972a0d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkConfig:\n",
    "    \"\"\"Configuration for text chunking.\n",
    "    \n",
    "    Attributes:\n",
    "        max_tokens: Maximum tokens per chunk (model's context window).\n",
    "        overlap_tokens: Number of tokens to overlap between chunks.\n",
    "        validate_integrity: If True, verify chunks match original text.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        max_tokens: int = 512,\n",
    "        safety_margin: int = 32,\n",
    "        overlap_tokens: int = 50,\n",
    "        max_lookback: int = 100,\n",
    "        validate_integrity: bool = True\n",
    "    ):\n",
    "        \"\"\"Initialize chunk configuration.\n",
    "        \n",
    "        Args:\n",
    "            max_tokens: Maximum tokens per chunk. For Arctic Embed, use 512.\n",
    "            overlap_tokens: Tokens to overlap between chunks for context.\n",
    "            validate_integrity: Whether to validate chunk integrity.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If configuration is invalid.\n",
    "        \"\"\"\n",
    "        if max_tokens <= 0:\n",
    "            raise ValueError(f'max_tokens must be positive, got {max_tokens}')\n",
    "        if safety_margin < 0:\n",
    "            raise safety_margin(f'safety_margin must be positive or 0, got {safety_margin}')\n",
    "        if overlap_tokens < 0:\n",
    "            raise ValueError(\n",
    "                f'overlap_tokens must be non-negative, got {overlap_tokens}'\n",
    "            )\n",
    "        if max_lookback <= 0:\n",
    "            raise ValueError(f'max_lookback must be positive, got {max_lookback}')\n",
    "        if overlap_tokens >= max_tokens:\n",
    "            raise ValueError(\n",
    "                f'overlap_tokens ({overlap_tokens}) must be less than '\n",
    "                f'max_tokens ({max_tokens})'\n",
    "            )\n",
    "        \n",
    "        self.max_tokens = max_tokens\n",
    "        self.safety_margin = safety_margin\n",
    "        self.overlap_tokens = overlap_tokens\n",
    "        self.max_lookback = max_lookback\n",
    "        self.validate_integrity = validate_integrity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f7721dc-24ce-42c9-869d-3e6581ad21ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Snowflake Arctic Embed model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1000 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLE 1: Basic Chunking\n",
      "======================================================================\n",
      "\n",
      "Document split into 1 chunks\n",
      "\n",
      "Chunk sizes:\n",
      "  Chunk 1: 181 tokens, 1070 characters\n",
      "\n",
      "First chunk preview:\n",
      "  machine learning is a subset of artificial intelligence that focuses on developing systems that can learn from and make decisions based on data. unlik...\n",
      "\n",
      "Generating embeddings for 1 chunks...\n",
      "Embeddings shape: (1, 384)\n",
      "Embedding dimension: 384\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 2: Custom Configuration\n",
      "======================================================================\n",
      "\n",
      "With smaller chunks: 1 chunks\n",
      "  Chunk 1: 181 tokens\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 3: Processing Multiple Documents\n",
      "======================================================================\n",
      "Document 1: 1 chunks\n",
      "Document 2: 1 chunks\n",
      "Document 3: 1 chunks\n",
      "\n",
      "Total chunks across all documents: 3\n",
      "Generated 3 embeddings\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 4: Integrity Validation in Action\n",
      "======================================================================\n",
      "✓ Integrity check passed for 3 chunks\n",
      "\n",
      "Empty text produces: []\n",
      "\n",
      "======================================================================\n",
      "Tutorial complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Simple, robust text chunking for BERT-based sentence transformers.\n",
    "\n",
    "This module provides text chunking with guaranteed integrity - every chunk\n",
    "can be perfectly reconstructed back to the original text. Designed for\n",
    "embedding models like Snowflake Arctic Embed that use BERT tokenization.\n",
    "\n",
    "Key features:\n",
    "- Guaranteed chunk integrity (lossless tokenization)\n",
    "- Word boundary-aware chunking\n",
    "- Configurable chunk size and overlap\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Tuple, Optional\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def chunk_text(\n",
    "    text: str,\n",
    "    tokenizer,\n",
    "    config: Optional[ChunkConfig] = None\n",
    ") -> List[str]:\n",
    "    \"\"\"Chunk a single text document with integrity validation.\n",
    "    \n",
    "    This function splits text into overlapping chunks that respect word\n",
    "    boundaries. Every chunk is validated to ensure it can be correctly\n",
    "    decoded back to the original text.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Tokenize the full document\n",
    "    2. Split tokens into overlapping chunks at word boundaries\n",
    "    3. Decode each chunk back to text\n",
    "    4. Validate that chunks can reconstruct the original text\n",
    "    \n",
    "    Args:\n",
    "        text: Text document to chunk.\n",
    "        tokenizer: HuggingFace BERT tokenizer (e.g., from Arctic Embed).\n",
    "        config: Chunking configuration. Defaults to 512 tokens, 50 overlap.\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks. Empty list if text is empty.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If chunking fails integrity validation.\n",
    "    \n",
    "    Example:\n",
    "        >>> from sentence_transformers import SentenceTransformer\n",
    "        >>> model = SentenceTransformer('Snowflake/snowflake-arctic-embed-s')\n",
    "        >>> chunks = chunk_text(long_document, model.tokenizer)\n",
    "        >>> embeddings = model.encode(chunks)\n",
    "    \"\"\"\n",
    "    config = config or ChunkConfig()\n",
    "    \n",
    "    # Handle empty documents\n",
    "    if not text or not text.strip():\n",
    "        return []\n",
    "\n",
    "    is_valid: bool = False\n",
    "    \n",
    "    # Tokenize the document\n",
    "    # BERT tokenizers handle all Unicode correctly\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        add_special_tokens=False,  # We don't want [CLS], [SEP]\n",
    "        truncation=False\n",
    "    )\n",
    "    \n",
    "    # Split tokens into chunks at word boundaries\n",
    "    token_ranges, is_valid = _create_chunks(tokens, tokenizer, config)\n",
    "    \n",
    "    # Decode chunks back to text\n",
    "    text_chunks = [\n",
    "        tokenizer.decode(\n",
    "            tokens[start:end], \n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        ).strip()\n",
    "        for start, end in token_ranges\n",
    "    ]\n",
    "    \n",
    "    # Remove empty chunks (can happen with whitespace-only sections)\n",
    "    text_chunks = [chunk for chunk in text_chunks if chunk]\n",
    "    \n",
    "    # Validate integrity: ensure chunks can reconstruct original\n",
    "    if config.validate_integrity:\n",
    "        _validate_integrity(text, tokens, token_ranges, tokenizer)\n",
    "    \n",
    "    return text_chunks\n",
    "\n",
    "\n",
    "def _create_chunks(\n",
    "    tokens: List[int],\n",
    "    tokenizer,\n",
    "    config: ChunkConfig\n",
    ") -> List[Tuple[int, int]]:\n",
    "    \"\"\"Split tokens into overlapping chunks with word boundaries.\n",
    "    \n",
    "    For BERT WordPiece tokenization:\n",
    "    - Tokens starting with \"##\" are word continuations\n",
    "    - Tokens NOT starting with \"##\" are word starts\n",
    "    - We try to break chunks at word starts for clean boundaries\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Start at position 0\n",
    "    2. Try to create chunk of max_tokens size\n",
    "    3. Look back up to 100 positions for a word boundary\n",
    "    4. If found, break there; otherwise use hard cut at max_tokens\n",
    "    5. Advance by (chunk_end - overlap) and repeat\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of token IDs.\n",
    "        tokenizer: Tokenizer instance (for decoding to check boundaries).\n",
    "        config: Chunking configuration.\n",
    "    \n",
    "    Returns:\n",
    "        List of (start, end) tuples representing chunk boundaries.\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return []\n",
    "    \n",
    "    token_ranges = []\n",
    "    start = 0\n",
    "    max_lookback = 100  # Look back up to 100 tokens for word boundary\n",
    "    is_valid: bool = False\n",
    "    \n",
    "    while start < len(tokens):\n",
    "        # Calculate initial chunk end position\n",
    "        end = min(start + config.max_tokens, len(tokens))\n",
    "        \n",
    "        # Try to find a word boundary if we're not at the document end\n",
    "        # This makes chunks more semantically coherent\n",
    "        if end < len(tokens):\n",
    "            # Look backwards from 'end' to find a good breaking point\n",
    "            for lookback in range(1, min(max_lookback, end - start) + 1):\n",
    "                test_pos = end - lookback\n",
    "                \n",
    "                # Decode the token at this position to check if it's a word start\n",
    "                # In WordPiece, tokens starting with \"##\" are continuations\n",
    "                token_text = tokenizer.decode(\n",
    "                    [tokens[test_pos]], skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "                )\n",
    "                \n",
    "                # Check if this is a word boundary\n",
    "                # Word boundaries: whitespace or tokens not starting with ##\n",
    "                if _is_word_boundary(token_text):\n",
    "                    end = test_pos\n",
    "                    break\n",
    "            \n",
    "            # If no boundary found in lookback window, use hard cut\n",
    "            # The integrity validation will ensure this is safe\n",
    "        \n",
    "        # Record this chunk's range\n",
    "        token_ranges.append((start, end))\n",
    "        \n",
    "        # Check if we're done processing the document\n",
    "        if end >= len(tokens):\n",
    "            break\n",
    "        \n",
    "        # Advance to next chunk with overlap\n",
    "        # This maintains context across chunk boundaries\n",
    "        next_start = end - config.overlap_tokens\n",
    "        \n",
    "        # Ensure we make forward progress (prevent infinite loop)\n",
    "        if next_start <= start:\n",
    "            raise ValueError(\n",
    "                f'Chunking failed to make progress: start={start}, '\n",
    "                f'next_start={next_start}, end={end}. '\n",
    "                f'Try reducing overlap_tokens or increasing max_tokens.'\n",
    "            )\n",
    "        \n",
    "        start = next_start\n",
    "\n",
    "    is_valid = True\n",
    "    return token_ranges, is_valid\n",
    "\n",
    "\n",
    "def _is_word_boundary(token_text: str) -> bool:\n",
    "    \"\"\"Check if a token represents a word boundary.\n",
    "    \n",
    "    For BERT WordPiece tokenization:\n",
    "    - Tokens starting with \"##\" are word continuations (not boundaries)\n",
    "    - Tokens with leading whitespace are word boundaries\n",
    "    - Tokens that are just whitespace are boundaries\n",
    "    \n",
    "    Args:\n",
    "        token_text: Decoded text of a single token.\n",
    "    \n",
    "    Returns:\n",
    "        True if this token represents a word boundary, False otherwise.\n",
    "    \"\"\"\n",
    "    if not token_text:\n",
    "        return False\n",
    "    \n",
    "    # Whitespace-only tokens are boundaries\n",
    "    if not token_text.strip():\n",
    "        return True\n",
    "    \n",
    "    # Tokens starting with whitespace are word boundaries\n",
    "    # (The whitespace indicates separation from previous word)\n",
    "    if token_text[0] in ' \\t\\n\\r':\n",
    "        return True\n",
    "    \n",
    "    # WordPiece continuation tokens start with ##\n",
    "    # These are NOT word boundaries\n",
    "    if token_text.startswith('##'):\n",
    "        return False\n",
    "    \n",
    "    # Everything else is considered a word boundary\n",
    "    return True\n",
    "\n",
    "\n",
    "def _validate_integrity(\n",
    "    original_text: str,\n",
    "    tokens: List[int],\n",
    "    token_ranges: List[Tuple[int, int]],\n",
    "    tokenizer\n",
    ") -> None:\n",
    "    \"\"\"Validate that chunks can perfectly reconstruct the original text.\n",
    "    \n",
    "    This is the critical integrity check that ensures no data loss during\n",
    "    the tokenization -> chunking -> decoding process.\n",
    "    \n",
    "    We verify two things:\n",
    "    1. All tokens decode back to the original text (checks tokenization)\n",
    "    2. Non-overlapping chunks reconstruct the original (checks chunking logic)\n",
    "    \n",
    "    Args:\n",
    "        original_text: Original input text before tokenization.\n",
    "        tokens: Token IDs from the original text.\n",
    "        token_ranges: List of (start, end) tuples for chunk boundaries.\n",
    "        tokenizer: Tokenizer instance for decoding.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If integrity check fails, with detailed error message\n",
    "                   explaining what went wrong.\n",
    "    \"\"\"\n",
    "    # Check 1: Verify full token sequence decodes to original text\n",
    "    # This ensures the tokenizer can handle this text correctly\n",
    "    full_decoded = tokenizer.decode(\n",
    "        tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "    )\n",
    "\n",
    "    # Normalize whitespace for comparison\n",
    "    # Tokenizers often normalize multiple spaces to single space\n",
    "    original_normalized = ''.join(unicodedata.normalize('NFKC',original_text).split()).lower()\n",
    "    decoded_normalized = ''.join(unicodedata.normalize('NFKC',full_decoded).split()).lower()\n",
    "    \n",
    "    if original_normalized != decoded_normalized:\n",
    "        # Calculate where the difference occurs for debugging\n",
    "        diff_pos = _find_first_diff(original_normalized, decoded_normalized)\n",
    "        raise ValueError(\n",
    "            f'Integrity check failed: decoded text does not match original.\\n'\n",
    "            f'Original length: {len(original_text)}\\n'\n",
    "            f'Decoded length: {len(full_decoded)}\\n'\n",
    "            f'First difference at position: {diff_pos}\\n'\n",
    "            f'This suggests the tokenizer cannot handle this text correctly.\\n'\n",
    "            f'Original excerpt: {original_normalized[max(0, diff_pos-20):diff_pos+20]}\\n'\n",
    "            f'Decoded excerpt: {decoded_normalized[max(0, diff_pos-20):diff_pos+20]}'\n",
    "        )\n",
    "    \n",
    "    # Check 2: Verify non-overlapping chunks reconstruct the original\n",
    "    # This ensures our chunking logic is correct\n",
    "    reconstructed_parts = []\n",
    "    prev_end = 0\n",
    "    \n",
    "    for start, end in token_ranges:\n",
    "        # Extract only the non-overlapping portion of this chunk\n",
    "        # If chunks overlap (start < prev_end), skip the overlapping tokens\n",
    "        if start < prev_end:\n",
    "            # This chunk overlaps with the previous chunk\n",
    "            # Only decode the new tokens: prev_end to end\n",
    "            chunk_tokens = tokens[prev_end:end]\n",
    "        else:\n",
    "            # No overlap, decode the full chunk\n",
    "            chunk_tokens = tokens[start:end]\n",
    "        \n",
    "        # Decode this chunk's tokens\n",
    "        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "        reconstructed_parts.append(chunk_text)\n",
    "        prev_end = end\n",
    "    \n",
    "    # Concatenate all non-overlapping chunks\n",
    "    reconstructed = unicodedata.normalize('NFKC', ' '.join(reconstructed_parts))\n",
    "    reconstructed_normalized = ''.join(reconstructed.split()).lower()\n",
    "    \n",
    "    if original_normalized != reconstructed_normalized:\n",
    "        diff_pos = _find_first_diff(original_normalized, reconstructed_normalized)\n",
    "        \n",
    "        raise ValueError(\n",
    "            f'Integrity check failed: reconstructed chunks do not match original.\\n'\n",
    "            f'Original length: {len(original_text)}\\n'\n",
    "            f'Reconstructed length: {len(reconstructed)}\\n'\n",
    "            f'Number of chunks: {len(token_ranges)}\\n'\n",
    "            f'First difference at position: {diff_pos}\\n'\n",
    "            f'This suggests a chunking logic error.\\n'\n",
    "            f'Original excerpt: {original_normalized[max(0, diff_pos-20):diff_pos+20]}\\n'\n",
    "            f'Reconstructed excerpt: {reconstructed_normalized[max(0, diff_pos-20):diff_pos+20]}'\n",
    "        )\n",
    "\n",
    "\n",
    "def _find_first_diff(str1: str, str2: str) -> int:\n",
    "    \"\"\"Find the position of the first character difference between strings.\n",
    "    \n",
    "    Helper function for generating detailed integrity validation errors.\n",
    "    \n",
    "    Args:\n",
    "        str1: First string to compare.\n",
    "        str2: Second string to compare.\n",
    "    \n",
    "    Returns:\n",
    "        Position (index) of first difference, or -1 if strings are equal.\n",
    "    \"\"\"\n",
    "    min_len = min(len(str1), len(str2))\n",
    "    \n",
    "    # Find first position where characters differ\n",
    "    for i in range(min_len):\n",
    "        if str1[i] != str2[i]:\n",
    "            return i\n",
    "    \n",
    "    # If we get here, one string is a prefix of the other\n",
    "    # The difference is at the end of the shorter string\n",
    "    if len(str1) != len(str2):\n",
    "        return min_len\n",
    "    \n",
    "    # Strings are identical\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c693a6a8-1e3c-4d2d-b3c7-a3ab775126f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf4f55c-1e35-4f76-bb7f-005791f613bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c82bdd5-c190-49d8-bced-bbbc3c6501f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bbc07a-9f87-4cdb-897d-7ed413c7e0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba2907e-2079-47de-a2d5-f49471156957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8d7db8e-8eb7-4789-bf5c-f323e7843d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Text chunking with batch tokenization support.\n",
    "\n",
    "This module provides efficient text chunking functionality with support for\n",
    "batch processing and multiple tokenizer types (BPE, SentencePiece, WordPiece).\n",
    "\n",
    "The chunking algorithm splits documents into token-based chunks while attempting\n",
    "to break at natural word boundaries rather than mid-word. It uses a hybrid\n",
    "approach: pre-decoding all tokens for small documents, and on-demand caching\n",
    "for large documents to balance memory usage and performance.\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Optional, Tuple, Callable\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Module-level constant for punctuation characters used in boundary detection\n",
    "# Using frozenset for O(1) lookup performance and immutability\n",
    "_PUNCTUATION_SET = frozenset('.,!?;:()[]{}\"\"\\'\\'`-–—…/\\\\|\\n\\t\\r')\n",
    "\n",
    "\n",
    "def chunk_texts_batch(\n",
    "    texts: List[str],\n",
    "    tokenizer,\n",
    "    config: Optional['ChunkConfig'] = None,\n",
    "    metrics: Optional['ChunkingMetrics'] = None,\n",
    "    batch_size: int = 32\n",
    ") -> List[Tuple[List[str], bool]]:\n",
    "    \"\"\"Batch process multiple documents with efficient tokenization.\n",
    "    \n",
    "    This function processes multiple documents in batches to leverage the\n",
    "    tokenizer's batch processing capabilities, which is 4-8x faster than\n",
    "    processing documents one at a time.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of documents to chunk.\n",
    "        tokenizer: HuggingFace tokenizer instance.\n",
    "        config: Chunking configuration. Defaults to ChunkConfig().\n",
    "        metrics: Optional metrics tracking object.\n",
    "        batch_size: Number of documents to tokenize at once. Larger batches\n",
    "            are faster but use more memory. Default 32 is a good balance.\n",
    "    \n",
    "    Returns:\n",
    "        List of (chunks, is_valid) tuples for each document, where:\n",
    "            - chunks: List of text chunks for the document\n",
    "            - is_valid: Boolean indicating if token coverage validation passed\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If any document produces invalid chunks.\n",
    "    \"\"\"\n",
    "    config = config or ChunkConfig()\n",
    "    results = []\n",
    "    \n",
    "    # Process documents in batches to leverage tokenizer's batch processing\n",
    "    # This is the key optimization: tokenizing 32 docs at once is much faster\n",
    "    # than tokenizing them one-by-one in a loop\n",
    "    for batch_start in range(0, len(texts), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(texts))\n",
    "        batch_texts = texts[batch_start:batch_end]\n",
    "        \n",
    "        # Batch tokenization: single call to tokenize multiple documents\n",
    "        # Key parameters:\n",
    "        #   - add_special_tokens=False: We don't want [CLS], [SEP], etc.\n",
    "        #   - truncation=False: We want to process the full document\n",
    "        #   - padding=False: No need to pad since we're not doing inference\n",
    "        #   - return_attention_mask=False: Not needed for chunking\n",
    "        try:\n",
    "            batch_encodings = tokenizer(\n",
    "                batch_texts,\n",
    "                add_special_tokens=False,\n",
    "                truncation=False,\n",
    "                return_attention_mask=False,\n",
    "                return_token_type_ids=False,\n",
    "                padding=False\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # Fallback: if document exceeds max length, truncate it\n",
    "            # This shouldn't happen often, but handles edge cases\n",
    "            logger.warning(\n",
    "                'Batch tokenization failed: %s, falling back to truncation', e\n",
    "            )\n",
    "            batch_encodings = tokenizer(\n",
    "                batch_texts,\n",
    "                add_special_tokens=False,\n",
    "                truncation=True,\n",
    "                return_attention_mask=False,\n",
    "                return_token_type_ids=False,\n",
    "                padding=False\n",
    "            )\n",
    "            if metrics:\n",
    "                metrics.truncation_warnings += len(batch_texts)\n",
    "        \n",
    "        # Process each document in the batch individually\n",
    "        # batch_encodings['input_ids'] is a list of token lists\n",
    "        for text, token_ids in zip(batch_texts, batch_encodings['input_ids']):\n",
    "            chunks, is_valid = _chunk_single_document(\n",
    "                text=text,\n",
    "                tokens=token_ids,\n",
    "                tokenizer=tokenizer,\n",
    "                config=config,\n",
    "                metrics=metrics\n",
    "            )\n",
    "            results.append((chunks, is_valid))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def detect_tokenizer_type(tokenizer) -> str:\n",
    "    \"\"\"Identify the underlying algorithm of the HuggingFace tokenizer.\"\"\"\n",
    "    name = str(type(tokenizer)).lower()\n",
    "    if \"bert\" in name: return \"wordpiece\"\n",
    "    if \"llama\" in name or \"sp\" in name: return \"sentencepiece\"\n",
    "    if \"gpt\" in name or \"roberta\" in name: return \"bpe\"\n",
    "    return \"generic\"\n",
    "\n",
    "    \n",
    "def _chunk_single_document(\n",
    "    text: str,\n",
    "    tokens: List[int],\n",
    "    tokenizer,\n",
    "    config: 'ChunkConfig',\n",
    "    metrics: Optional['ChunkingMetrics'] = None\n",
    ") -> Tuple[List[str], bool]:\n",
    "    \"\"\"Chunk a single document using pre-tokenized tokens.\n",
    "    \n",
    "    This function is separated from chunk_text_hybrid to enable batch\n",
    "    tokenization. It uses a hybrid strategy: simple list comprehension for\n",
    "    short documents, on-demand caching for long documents.\n",
    "    \n",
    "    Core algorithm:\n",
    "    1. Start at position 0\n",
    "    2. Try to create chunk of size effective_limit\n",
    "    3. Look back up to max_lookback positions for a word boundary\n",
    "    4. If found, use that position; otherwise, hard cut at effective_limit\n",
    "    5. Advance by (chunk_end - overlap) and repeat until document is covered\n",
    "    \n",
    "    Args:\n",
    "        text: Original text of the document (used for validation).\n",
    "        tokens: Pre-tokenized token IDs from the document.\n",
    "        tokenizer: HuggingFace tokenizer instance.\n",
    "        config: Chunking configuration with max_tokens, overlap, etc.\n",
    "        metrics: Optional metrics tracking object.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (chunks, is_valid) where:\n",
    "            - chunks: List of text chunks\n",
    "            - is_valid: Boolean indicating validation success\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If zero-length chunks are detected or no progress is made.\n",
    "    \"\"\"\n",
    "    # Handle empty documents early\n",
    "    if not text or not text.strip():\n",
    "        return [], True\n",
    "    \n",
    "    # Detect tokenizer type to use appropriate word boundary markers\n",
    "    # Different tokenizers use different conventions for word starts\n",
    "    tokenizer_type = detect_tokenizer_type(tokenizer)\n",
    "    \n",
    "    # Calculate effective chunk size accounting for safety margin\n",
    "    # Safety margin prevents edge cases where token count != character count\n",
    "    effective_limit = config.max_tokens - config.safety_margin\n",
    "    \n",
    "    # Hybrid strategy: choose decoding method based on document size\n",
    "    # For small docs, pre-decode everything (simple, fast)\n",
    "    # For large docs, decode on-demand with caching (memory-efficient)\n",
    "    token_threshold = 10_000\n",
    "    get_token_text = _create_token_text_getter(\n",
    "        tokens, tokenizer, token_threshold\n",
    "    )\n",
    "    \n",
    "    # Track chunks and their token ranges for validation\n",
    "    chunks = []\n",
    "    chunk_token_ranges = []  # List of (start, end) tuples\n",
    "    start = 0\n",
    "    \n",
    "    # Main chunking loop: process tokens from start to end\n",
    "    while start < len(tokens):\n",
    "        # Initial end position: start + chunk size, capped at document end\n",
    "        end = min(start + effective_limit, len(tokens))\n",
    "        boundary_found = False\n",
    "        \n",
    "        # Word boundary search: only needed if we're not at the end\n",
    "        # We look backwards from 'end' to find a good breaking point\n",
    "        if end < len(tokens):\n",
    "            # Limit lookback to avoid excessive searching\n",
    "            # Using 1/5 of chunk size as max prevents long searches\n",
    "            max_lookback = min(config.max_lookback, effective_limit // 5)\n",
    "            \n",
    "            # Search backwards from 'end' for a word boundary\n",
    "            # Start at lookback=1 (not 0) to check position end-1 first\n",
    "            for lookback in range(1, max_lookback + 1):\n",
    "                test_end = end - lookback\n",
    "                \n",
    "                # Stop if we've looked back to the start position\n",
    "                # This prevents creating zero-length chunks\n",
    "                if test_end <= start:\n",
    "                    break\n",
    "                \n",
    "                # Decode token at test position to check if it's a boundary\n",
    "                token_text = get_token_text(test_end)\n",
    "                is_boundary, boundary_type = _is_word_boundary_fast(\n",
    "                    token_text, tokenizer_type, _PUNCTUATION_SET\n",
    "                )\n",
    "                \n",
    "                # If boundary found, use this position and stop searching\n",
    "                if is_boundary:\n",
    "                    end = test_end\n",
    "                    boundary_found = True\n",
    "                    if metrics:\n",
    "                        # Track what type of boundary we found\n",
    "                        metrics.boundary_types[boundary_type] += 1\n",
    "                    break\n",
    "        \n",
    "        # Handle fallback: no boundary found, use hard cut\n",
    "        # This happens when no good breaking point exists in lookback window\n",
    "        if not boundary_found and end < len(tokens):\n",
    "            if metrics:\n",
    "                metrics.boundary_fallbacks += 1\n",
    "            if config.warn_on_fallback:\n",
    "                logger.warning(\n",
    "                    'No boundary found (%d-%d). Using hard cut.', start, end\n",
    "                )\n",
    "        \n",
    "        # Extract tokens for this chunk\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        \n",
    "        # Sanity check: ensure we're making progress\n",
    "        # Zero-length chunks indicate a logic error\n",
    "        if not start < end:\n",
    "            raise ValueError(\n",
    "                f'Zero-length chunk detected: start={start}, end={end}'\n",
    "            )\n",
    "        \n",
    "        # Decode chunk tokens back to text\n",
    "        # skip_special_tokens=True removes [PAD], [UNK], etc.\n",
    "        # .strip() removes leading/trailing whitespace\n",
    "        chunk_str = tokenizer.decode(\n",
    "            chunk_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        ).strip()\n",
    "        \n",
    "        # Only add non-empty chunks\n",
    "        # Empty chunks can occur with certain token sequences\n",
    "        if chunk_str:\n",
    "            if metrics:\n",
    "                metrics.total_chunks += 1\n",
    "                # Track micro-chunks (chunks much smaller than limit)\n",
    "                chunk_ratio = len(chunk_tokens) / effective_limit\n",
    "                if chunk_ratio < config.micro_chunk_threshold:\n",
    "                    metrics.micro_chunks += 1\n",
    "            \n",
    "            chunks.append(chunk_str)\n",
    "            chunk_token_ranges.append((start, end))\n",
    "        \n",
    "        # Check if we've processed the entire document\n",
    "        if end >= len(tokens):\n",
    "            break\n",
    "        \n",
    "        # Advance to next chunk with overlap\n",
    "        # Overlap helps maintain context across chunk boundaries\n",
    "        # Example: if end=500 and overlap=50, next_start=450\n",
    "        # This means tokens 450-500 appear in both chunks\n",
    "        next_start = end - config.overlap_tokens\n",
    "        \n",
    "        # Sanity check: ensure we're making forward progress\n",
    "        # If next_start <= start, we'd be stuck in an infinite loop\n",
    "        if next_start <= start and end < len(tokens):\n",
    "            raise ValueError(\n",
    "                f'No progress made: start={start}, next_start={next_start}, '\n",
    "                f'end={end}, total={len(tokens)}'\n",
    "            )\n",
    "        \n",
    "        start = next_start\n",
    "    \n",
    "    # Validate that chunks cover all tokens without gaps or skips\n",
    "    # This is a fast O(c) check where c is the number of chunks\n",
    "    is_valid = validate_token_coverage_fast(tokens, chunk_token_ranges)\n",
    "    \n",
    "    if not is_valid and metrics:\n",
    "        metrics.validation_failures += 1\n",
    "    \n",
    "    # NEW: Validate chunk integrity - ensures decoded chunks match original\n",
    "    # This checks that no information is lost during tokenization/decoding\n",
    "    integrity_valid, error_msg = validate_chunk_integrity(\n",
    "        original_text=text,\n",
    "        tokens=tokens,\n",
    "        chunk_token_ranges=chunk_token_ranges,\n",
    "        tokenizer=tokenizer,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    if not integrity_valid:\n",
    "        logger.error('Chunk integrity check failed: %s', error_msg)\n",
    "        if metrics:\n",
    "            metrics.integrity_failures += 1\n",
    "        # Mark overall validation as failed\n",
    "        is_valid = False\n",
    "    \n",
    "    return chunks, is_valid\n",
    "\n",
    "\n",
    "def _create_token_text_getter(\n",
    "    tokens: List[int], tokenizer, threshold: int\n",
    ") -> Callable[[int], str]:\n",
    "    \"\"\"Create a token text getter function based on document size.\n",
    "    \n",
    "    This implements the hybrid strategy:\n",
    "    - Short documents: Pre-decode all tokens into a list (O(1) access)\n",
    "    - Long documents: Decode on-demand with caching (memory-efficient)\n",
    "    \n",
    "    The threshold-based decision balances speed vs memory:\n",
    "    - Below 10K tokens: ~100KB memory, instant access\n",
    "    - Above 10K tokens: Minimal memory, slight decoding overhead\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of token IDs.\n",
    "        tokenizer: HuggingFace tokenizer instance.\n",
    "        threshold: Token count threshold for strategy selection.\n",
    "    \n",
    "    Returns:\n",
    "        Function that takes token index and returns decoded text.\n",
    "    \"\"\"\n",
    "    if len(tokens) < threshold:\n",
    "        # SHORT DOCUMENTS: Pre-decode all tokens\n",
    "        # This creates a list where token_texts[i] = decoded text of tokens[i]\n",
    "        # Memory: O(n) where n is number of tokens\n",
    "        # Access time: O(1) - simple list lookup\n",
    "        token_texts = [\n",
    "            tokenizer.decode([tok], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "            for tok in tokens\n",
    "        ]\n",
    "        return lambda idx: token_texts[idx]\n",
    "    else:\n",
    "        # LONG DOCUMENTS: On-demand decoding with caching\n",
    "        # Only decode tokens when needed, cache results\n",
    "        # Memory: O(k) where k is number of accessed tokens (much less than n)\n",
    "        # Access time: O(1) for cached, O(decode) for first access\n",
    "        decode_cache = {}\n",
    "        cache_size_limit = 5000\n",
    "        \n",
    "        def get_token_text(idx: int) -> str:\n",
    "            \"\"\"Get decoded text for token at index with caching.\n",
    "            \n",
    "            Cache eviction strategy: Simple clear when limit reached.\n",
    "            This is not LRU, but works well because we access tokens\n",
    "            sequentially (during lookback) so recently used tokens\n",
    "            will be re-cached if needed.\n",
    "            \"\"\"\n",
    "            if idx not in decode_cache:\n",
    "                # Prevent unbounded cache growth\n",
    "                # When cache gets too large, clear it completely\n",
    "                # This is OK because we typically access tokens in sequence\n",
    "                if len(decode_cache) >= cache_size_limit:\n",
    "                    decode_cache.clear()\n",
    "                    \n",
    "                # Decode single token and cache result\n",
    "                decode_cache[idx] = tokenizer.decode(\n",
    "                    [tokens[idx]], skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "                )\n",
    "            return decode_cache[idx]\n",
    "        \n",
    "        return get_token_text\n",
    "\n",
    "\n",
    "def _is_word_boundary_fast(\n",
    "    token_text: str,\n",
    "    tokenizer_type: str,\n",
    "    punctuation_set: frozenset\n",
    ") -> Tuple[bool, Optional[str]]:\n",
    "    \"\"\"Fast boundary check for word tokenization.\n",
    "    \n",
    "    Different tokenizers use different conventions for marking word starts:\n",
    "    - BPE (GPT): Uses Ġ (U+0120) for space before word\n",
    "    - SentencePiece (LLaMA/T5): Uses ▁ (U+2581) for space before word\n",
    "    - WordPiece (BERT): Tokens NOT starting with ## are word starts\n",
    "    \n",
    "    Checks in order of likelihood for performance:\n",
    "    1. Whitespace (most common)\n",
    "    2. Tokenizer-specific markers\n",
    "    3. Punctuation\n",
    "    \n",
    "    Args:\n",
    "        token_text: Decoded text of the token to check.\n",
    "        tokenizer_type: Type of tokenizer ('bpe', 'sentencepiece', 'wordpiece').\n",
    "        punctuation_set: Set of punctuation characters for O(1) lookup.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (is_boundary, boundary_type) where:\n",
    "            - is_boundary: Boolean indicating if this is a word boundary\n",
    "            - boundary_type: String description of boundary type or None\n",
    "    \"\"\"\n",
    "    # Empty tokens are not boundaries\n",
    "    if not token_text:\n",
    "        return False, None\n",
    "    \n",
    "    # Whitespace-only tokens are boundaries\n",
    "    # Example: \"   \" -> boundary\n",
    "    if not token_text.strip():\n",
    "        return True, 'whitespace'\n",
    "    \n",
    "    first_char = token_text[0]\n",
    "    \n",
    "    # Fast path: Check for whitespace at start\n",
    "    # Common in most tokenizers\n",
    "    if first_char in ' \\t\\n\\r':\n",
    "        return True, 'whitespace'\n",
    "    \n",
    "    # Tokenizer-specific word start markers\n",
    "    # These are special Unicode characters used by different tokenizers\n",
    "    \n",
    "    # BPE (GPT models): Ġ indicates space before word\n",
    "    # Example token: \"Ġhello\" represents \" hello\"\n",
    "    if tokenizer_type == 'bpe' and first_char == '\\u0120':\n",
    "        return True, 'bpe_start'\n",
    "    \n",
    "    # SentencePiece (LLaMA, T5): ▁ indicates space before word\n",
    "    # Example token: \"▁hello\" represents \" hello\"\n",
    "    if tokenizer_type == 'sentencepiece' and first_char == '\\u2581':\n",
    "        return True, 'sentencepiece_start'\n",
    "    \n",
    "    # WordPiece (BERT): Tokens starting with ## are word continuations\n",
    "    # So tokens NOT starting with ## are word starts\n",
    "    # Example: \"hello\" is start, \"##ing\" is continuation\n",
    "    if tokenizer_type == 'wordpiece' and not token_text.startswith('##'):\n",
    "        return True, 'wordpiece_start'\n",
    "    \n",
    "    # Check if token starts with punctuation (after stripping whitespace)\n",
    "    # Example: \" .\" or \".\" both count as punctuation boundary\n",
    "    # We strip first to handle tokens like \"  ,\"\n",
    "    stripped = token_text.lstrip()\n",
    "    if stripped and stripped[0] in punctuation_set:\n",
    "        return True, 'punctuation'\n",
    "    \n",
    "    # Not a boundary: this token is in the middle of a word\n",
    "    return False, None\n",
    "\n",
    "\n",
    "def validate_token_coverage_fast(\n",
    "    tokens: List[int],\n",
    "    ranges: List[Tuple[int, int]]\n",
    ") -> bool:\n",
    "    \"\"\"Fast O(c) validation of token coverage.\n",
    "    \n",
    "    This function validates that chunks completely cover the document without\n",
    "    gaps or skips. It allows overlaps (which are intentional for context).\n",
    "    \n",
    "    Algorithm complexity: O(c) where c is number of chunks, NOT O(n) where\n",
    "    n is number of tokens. This is much faster for large documents.\n",
    "    \n",
    "    Example valid coverage (with overlap):\n",
    "    tokens: 0...100\n",
    "    ranges: [(0, 50), (40, 90), (80, 100)]\n",
    "    - Starts at 0 ✓\n",
    "    - Ends at 100 ✓\n",
    "    - No gaps: 40 < 50, 80 < 90 ✓\n",
    "    - Overlaps are fine: 40-50 appears in both chunks\n",
    "    \n",
    "    Example invalid coverage (with gap):\n",
    "    ranges: [(0, 50), (60, 100)]\n",
    "    - Gap from 50-60: next_start (60) > prev_end (50) ✗\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of token IDs from the original document.\n",
    "        ranges: List of (start, end) tuples representing chunk boundaries.\n",
    "    \n",
    "    Returns:\n",
    "        True if coverage is valid, False otherwise.\n",
    "    \n",
    "    Checks performed:\n",
    "        1. First chunk starts at index 0\n",
    "        2. Last chunk ends at len(tokens)\n",
    "        3. No gaps between consecutive chunks (overlaps are allowed)\n",
    "    \"\"\"\n",
    "    # Empty document edge case\n",
    "    if not ranges:\n",
    "        return len(tokens) == 0\n",
    "    \n",
    "    # Check 1: First chunk must start at the beginning\n",
    "    # If it doesn't, we're missing tokens 0...(first_start-1)\n",
    "    if ranges[0][0] != 0:\n",
    "        return False\n",
    "    \n",
    "    # Check 2: Last chunk must end at the document end\n",
    "    # If it doesn't, we're missing tokens (last_end)...len(tokens)-1\n",
    "    if ranges[-1][1] != len(tokens):\n",
    "        return False\n",
    "    \n",
    "    # Check 3: No gaps between consecutive chunks\n",
    "    # For each pair of adjacent chunks, verify the next chunk starts\n",
    "    # at or before the previous chunk ended\n",
    "    # If next_start > prev_end, there's a gap of uncovered tokens\n",
    "    for i in range(len(ranges) - 1):\n",
    "        current_end = ranges[i][1]\n",
    "        next_start = ranges[i + 1][0]\n",
    "        \n",
    "        # Gap detected: tokens from current_end to next_start are missing\n",
    "        if next_start > current_end:\n",
    "            return False\n",
    "    \n",
    "    # All checks passed: complete coverage with no gaps\n",
    "    return True\n",
    "\n",
    "\n",
    "def validate_chunk_integrity(\n",
    "    original_text: str,\n",
    "    tokens: List[int],\n",
    "    chunk_token_ranges: List[Tuple[int, int]],\n",
    "    tokenizer,\n",
    "    config: 'ChunkConfig'\n",
    ") -> Tuple[bool, Optional[str]]:\n",
    "    \"\"\"Validate that chunks can reconstruct the original text.\n",
    "    \n",
    "    This critical validation ensures no information is lost during the\n",
    "    tokenization -> chunking -> decoding process. It checks both:\n",
    "    1. Full token sequence decodes back to original text\n",
    "    2. Non-overlapping chunk reconstruction matches original\n",
    "    \n",
    "    Common failure modes detected:\n",
    "    - Lossy Unicode tokenization (e.g., rare emoji, mathematical symbols)\n",
    "    - Special token handling issues\n",
    "    - Encoding/decoding mismatches\n",
    "    - Whitespace normalization problems\n",
    "    \n",
    "    Args:\n",
    "        original_text: Original input text before tokenization.\n",
    "        tokens: Token IDs from the original text.\n",
    "        chunk_token_ranges: List of (start, end) tuples for chunks.\n",
    "        tokenizer: HuggingFace tokenizer instance.\n",
    "        config: Chunking configuration (for integrity_check settings).\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (is_valid, error_message):\n",
    "            - is_valid: True if integrity check passes\n",
    "            - error_message: Description of mismatch if validation fails,\n",
    "                           None if validation passes\n",
    "    \n",
    "    Note:\n",
    "        This validation allows whitespace differences by default since\n",
    "        tokenizers often normalize whitespace (multiple spaces -> single space).\n",
    "        Set config.strict_integrity_check=True to require exact matches.\n",
    "    \"\"\"\n",
    "    # Check 1: Verify full token sequence decodes correctly\n",
    "    # This catches fundamental tokenization issues\n",
    "    try:\n",
    "        full_decoded = tokenizer.decode(\n",
    "            tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return False, f'Failed to decode tokens: {e}'\n",
    "    \n",
    "    # Normalize texts for comparison\n",
    "    # Most tokenizers normalize whitespace, so we do the same\n",
    "    # This prevents false failures due to \"\\n\\n\" -> \"\\n\" normalization\n",
    "    original_normalized = ''.join(original_text.split()).lower()\n",
    "    decoded_normalized = ''.join(full_decoded.split()).lower()\n",
    "    \n",
    "    # Allow optional strict mode for exact matching\n",
    "    strict_mode = getattr(config, 'strict_integrity_check', False)\n",
    "    \n",
    "    if strict_mode:\n",
    "        # Strict mode: require exact match (only strip outer whitespace)\n",
    "            return False, (\n",
    "                f'Strict integrity check failed. '\n",
    "                f'Original length: {len(original_text)}, '\n",
    "                f'Decoded length: {len(full_decoded)}, '\n",
    "                f'First difference at position: '\n",
    "                f'{_find_first_diff(original_text, full_decoded)}'\n",
    "            )\n",
    "    else:\n",
    "        # Normal mode: allow whitespace normalization\n",
    "        if original_normalized != decoded_normalized:\n",
    "            return False, (\n",
    "                f'Decoded text does not match original after normalization. '\n",
    "                f'Original normalized length: {len(original_normalized)}, '\n",
    "                f'Decoded normalized length: {len(decoded_normalized)}, '\n",
    "                f'First difference at position: '\n",
    "                f'{_find_first_diff(original_normalized, decoded_normalized)}'\n",
    "            )\n",
    "    \n",
    "    # Check 2: Verify non-overlapping chunk reconstruction\n",
    "    # This ensures chunk boundaries are correct and don't skip tokens\n",
    "    reconstructed_parts = []\n",
    "    prev_end = 0\n",
    "    \n",
    "    for start, end in chunk_token_ranges:\n",
    "        # Extract only the non-overlapping portion of this chunk\n",
    "        # If chunks overlap (start < prev_end), skip the overlapping tokens\n",
    "        if start < prev_end:\n",
    "            # This chunk overlaps with previous chunk\n",
    "            # Only decode the new tokens: prev_end to end\n",
    "            chunk_tokens = tokens[prev_end:end]\n",
    "        else:\n",
    "            # No overlap, decode the full chunk\n",
    "            chunk_tokens = tokens[start:end]\n",
    "        \n",
    "        # Decode this chunk's tokens\n",
    "        try:\n",
    "            chunk_text = tokenizer.decode(\n",
    "                chunk_tokens, skip_special_tokens=True\n",
    "            )\n",
    "            reconstructed_parts.append(chunk_text)\n",
    "        except Exception as e:\n",
    "            return False, (\n",
    "                f'Failed to decode chunk tokens [{start}:{end}]: {e}'\n",
    "            )\n",
    "        \n",
    "        prev_end = end\n",
    "    \n",
    "    # Concatenate all non-overlapping chunks\n",
    "    # This should equal the original text (modulo whitespace)\n",
    "    reconstructed = ' '.join(reconstructed_parts)\n",
    "    reconstructed_normalized = ''.join(reconstructed.split())\n",
    "    \n",
    "    if strict_mode:\n",
    "        if original_text.strip() != reconstructed.strip():\n",
    "            return False, (\n",
    "                f'Reconstructed chunks do not match original (strict mode). '\n",
    "                f'Reconstruction length: {len(reconstructed)}'\n",
    "            )\n",
    "    else:\n",
    "        if original_normalized != reconstructed_normalized:\n",
    "            return False, (\n",
    "                f'Reconstructed chunks do not match original. '\n",
    "                f'Original normalized length: {len(original_normalized)}, '\n",
    "                f'Reconstructed normalized length: '\n",
    "                f'{len(reconstructed_normalized)}, '\n",
    "                f'First difference at position: '\n",
    "                f'{_find_first_diff(original_normalized, reconstructed_normalized)}'\n",
    "            )\n",
    "    \n",
    "    # Both checks passed: integrity is valid\n",
    "    return True, None\n",
    "\n",
    "\n",
    "def _find_first_diff(str1: str, str2: str) -> int:\n",
    "    \"\"\"Find the position of the first character difference between strings.\n",
    "    \n",
    "    Helper function for integrity validation error messages.\n",
    "    \n",
    "    Args:\n",
    "        str1: First string to compare.\n",
    "        str2: Second string to compare.\n",
    "    \n",
    "    Returns:\n",
    "        Position (index) of first difference, or -1 if strings are equal.\n",
    "    \"\"\"\n",
    "    min_len = min(len(str1), len(str2))\n",
    "    \n",
    "    for i in range(min_len):\n",
    "        if str1[i] != str2[i]:\n",
    "            return i\n",
    "    \n",
    "    # If we get here, one string is a prefix of the other\n",
    "    # The difference is at the end of the shorter string\n",
    "    if len(str1) != len(str2):\n",
    "        return min_len\n",
    "    \n",
    "    # Strings are identical\n",
    "    return -1\n",
    "\n",
    "\n",
    "def chunk_text(\n",
    "    text: str,\n",
    "    tokenizer,\n",
    "    config: Optional['ChunkConfig'] = None,\n",
    "    metrics: Optional['ChunkingMetrics'] = None\n",
    ") -> Tuple[List[str], bool]:\n",
    "    \"\"\"Chunk a single text document with hybrid optimization strategy.\n",
    "    \n",
    "    This is the main entry point for single-document chunking. For better\n",
    "    performance when processing multiple documents, use chunk_texts_batch()\n",
    "    which leverages batch tokenization (4-8x faster).\n",
    "    \n",
    "    Args:\n",
    "        text: Text document to chunk.\n",
    "        tokenizer: HuggingFace tokenizer instance.\n",
    "        config: Chunking configuration. Defaults to ChunkConfig().\n",
    "        metrics: Optional metrics tracking object.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (chunks, is_valid) where:\n",
    "            - chunks: List of text chunks\n",
    "            - is_valid: Boolean indicating validation success\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If chunking produces invalid results.\n",
    "    \"\"\"\n",
    "    config = config or ChunkConfig()\n",
    "    \n",
    "    # Handle empty documents early\n",
    "    if not text or not text.strip():\n",
    "        return [], True\n",
    "    \n",
    "    # Tokenize the document\n",
    "    # We try without truncation first, fall back to truncation if needed\n",
    "    try:\n",
    "        tokens = tokenizer.encode(\n",
    "            text, add_special_tokens=False, truncation=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Fallback: truncate if document exceeds tokenizer's max length\n",
    "        logger.warning('Tokenization error: %s, using truncation', e)\n",
    "        tokens = tokenizer.encode(\n",
    "            text, add_special_tokens=False, truncation=True\n",
    "        )\n",
    "        if metrics:\n",
    "            metrics.truncation_warnings += 1\n",
    "    \n",
    "    # Delegate to the main chunking logic\n",
    "    return _chunk_single_document(text, tokens, tokenizer, config, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93db8067-08c9-4657-aa12-20977260ecea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c469b-bd2a-42ef-b9b9-15e68ae2693a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5157b9-da80-4c36-a94b-9dc4d47467f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "183e38fb-350b-4e0c-ac5b-bc45e936cb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Snowflake Arctic Embed model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1000 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLE 1: Basic Chunking\n",
      "======================================================================\n",
      "\n",
      "Document split into 1 chunks\n",
      "\n",
      "Chunk sizes:\n",
      "  Chunk 1: 181 tokens, 1070 characters\n",
      "\n",
      "First chunk preview:\n",
      "  machine learning is a subset of artificial intelligence that focuses on developing systems that can learn from and make decisions based on data. unlik...\n",
      "\n",
      "Generating embeddings for 1 chunks...\n",
      "Embeddings shape: (1, 384)\n",
      "Embedding dimension: 384\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 2: Custom Configuration\n",
      "======================================================================\n",
      "\n",
      "With smaller chunks: 1 chunks\n",
      "  Chunk 1: 181 tokens\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 3: Processing Multiple Documents\n",
      "======================================================================\n",
      "Document 1: 1 chunks\n",
      "Document 2: 1 chunks\n",
      "Document 3: 1 chunks\n",
      "\n",
      "Total chunks across all documents: 3\n",
      "Generated 3 embeddings\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 4: Integrity Validation in Action\n",
      "======================================================================\n",
      "✓ Integrity check passed for 3 chunks\n",
      "\n",
      "Empty text produces: ([], True)\n",
      "\n",
      "======================================================================\n",
      "Tutorial complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example usage and tutorial\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    Tutorial: Using chunking with Snowflake Arctic Embed\n",
    "    \n",
    "    This example shows how to chunk long documents and embed them using\n",
    "    the Snowflake Arctic Embed sentence transformer model.\n",
    "    \"\"\"\n",
    "    print(\"Loading Snowflake Arctic Embed model...\")\n",
    "    model = SentenceTransformer('Snowflake/snowflake-arctic-embed-s')\n",
    "    \n",
    "    # Example long document (in practice, this would be much longer)\n",
    "    long_document = \"\"\"\n",
    "    Machine learning is a subset of artificial intelligence that focuses on\n",
    "    developing systems that can learn from and make decisions based on data.\n",
    "    Unlike traditional programming where rules are explicitly coded, machine\n",
    "    learning algorithms build models based on sample data, known as training data.\n",
    "    \n",
    "    The field of machine learning is closely related to computational statistics,\n",
    "    which focuses on making predictions using computers. It has strong ties to\n",
    "    mathematical optimization, which delivers methods, theory and application\n",
    "    domains to the field.\n",
    "    \n",
    "    Deep learning, a subset of machine learning, uses neural networks with\n",
    "    multiple layers to progressively extract higher-level features from raw input.\n",
    "    For example, in image processing, lower layers may identify edges, while\n",
    "    higher layers may identify concepts relevant to a human such as digits,\n",
    "    letters, or faces.\n",
    "    \n",
    "    Modern machine learning has many applications including computer vision,\n",
    "    speech recognition, email filtering, agriculture, and medicine. When applied\n",
    "    to business problems, it is known as predictive analytics.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 1: Basic Chunking\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Configure chunking for Arctic Embed\n",
    "    # Arctic Embed has a 512 token context window\n",
    "    config = ChunkConfig(\n",
    "        max_tokens=512,          # Model's maximum context length\n",
    "        overlap_tokens=50,       # Overlap for context continuity\n",
    "        validate_integrity=True  # Always validate! (default)\n",
    "    )\n",
    "    is_valid: bool = False\n",
    "    \n",
    "    # Chunk the document\n",
    "    chunks, is_valid = chunk_text(long_document, model.tokenizer, config)\n",
    "    \n",
    "    print(f\"\\nDocument split into {len(chunks)} chunks\")\n",
    "    print(f\"\\nChunk sizes:\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        tokens = model.tokenizer.encode(chunk, add_special_tokens=False)\n",
    "        print(f\"  Chunk {i+1}: {len(tokens)} tokens, {len(chunk)} characters\")\n",
    "    \n",
    "    print(f\"\\nFirst chunk preview:\")\n",
    "    print(f\"  {chunks[0][:150]}...\")\n",
    "    \n",
    "    # Embed all chunks\n",
    "    print(f\"\\nGenerating embeddings for {len(chunks)} chunks...\")\n",
    "    embeddings = model.encode(chunks)\n",
    "    \n",
    "    print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "    print(f\"Embedding dimension: {embeddings[0].shape[0]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 2: Custom Configuration\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Smaller chunks with more overlap for fine-grained search\n",
    "    small_config = ChunkConfig(\n",
    "        max_tokens=256,      # Smaller chunks\n",
    "        overlap_tokens=100,  # More overlap for better context\n",
    "        validate_integrity=True\n",
    "    )\n",
    "    \n",
    "    small_chunks, is_valid = chunk_text(long_document, model.tokenizer, small_config)\n",
    "    \n",
    "    print(f\"\\nWith smaller chunks: {len(small_chunks)} chunks\")\n",
    "    for i, chunk in enumerate(small_chunks):\n",
    "        tokens = model.tokenizer.encode(chunk, add_special_tokens=False)\n",
    "        print(f\"  Chunk {i+1}: {len(tokens)} tokens\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 3: Processing Multiple Documents\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    documents = [\n",
    "        \"First document about AI and machine learning...\",\n",
    "        \"Second document about deep learning architectures...\",\n",
    "        \"Third document about natural language processing...\"\n",
    "    ]\n",
    "    \n",
    "    all_chunks = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        doc_chunks, is_valid = chunk_text(doc, model.tokenizer, config)\n",
    "        print(f\"Document {i+1}: {len(doc_chunks)} chunks\")\n",
    "        all_chunks.extend(doc_chunks)\n",
    "    \n",
    "    print(f\"\\nTotal chunks across all documents: {len(all_chunks)}\")\n",
    "    \n",
    "    # Embed all chunks\n",
    "    all_embeddings = model.encode(all_chunks)\n",
    "    print(f\"Generated {len(all_embeddings)} embeddings\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXAMPLE 4: Integrity Validation in Action\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # This will succeed\n",
    "    try:\n",
    "        test_text = \"The quick brown fox jumps over the lazy dog. \" * 100\n",
    "        test_chunks, is_valid = chunk_text(test_text, model.tokenizer, config)\n",
    "        print(f\"✓ Integrity check passed for {len(test_chunks)} chunks\")\n",
    "    except ValueError as e:\n",
    "        print(f\"✗ Integrity check failed: {e}\")\n",
    "    \n",
    "    # Example: What happens with empty text\n",
    "    empty_chunks = chunk_text(\"\", model.tokenizer, config)\n",
    "    print(f\"\\nEmpty text produces: {empty_chunks}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Tutorial complete!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a4fd2b-9a93-4c33-9dff-2a2d41d604aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78bf9c2-eaae-427a-88f5-2c567db66d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad6b62-2722-429a-a57f-9a54d10429f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
