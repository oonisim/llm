{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847de34b-45bb-4c5f-81c3-d684e2628809",
   "metadata": {},
   "source": [
    "# LangGrpah Chatbot\n",
    "\n",
    "* Real Pyton - [Build Stateful AI Agents in Python](https://realpython.com/langgraph-python/)\n",
    "\n",
    "> LangGraph expands LangChain’s capabilities by providing tools to build complex LLM workflows with state, conditional edges, and cycles.\n",
    "\n",
    "\n",
    "* [docs/tutorials/get-started](https://github.com/langchain-ai/langgraph/tree/main/docs/docs/tutorials/get-started)\n",
    "\n",
    "> To get acquainted with LangGraph's key concepts and features, complete the following LangGraph basics tutorials series:\n",
    "> \n",
    "> 1. Build a basic chatbot\n",
    "> 2. Add tools\n",
    "> 3. Add memory\n",
    "> 4. Add human-in-the-loop controls\n",
    "> 5. Customize state\n",
    "> 6. Time travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be572eff-d932-479f-aa9e-ca37649ea842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Annotated, List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "import openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3fc920-71a5-4002-8385-bf34997e00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"OpenAI chat model listing and connection test using LangChain.\n",
    "\n",
    "This script:\n",
    "1. Lists all chat-compatible OpenAI models.\n",
    "2. Picks the latest GPT model suitable for chat.\n",
    "3. Tests the selected model with a simple prompt.\n",
    "\n",
    "Requires:\n",
    "    - langchain-openai\n",
    "    - langchain-core\n",
    "    - openai >= 1.0.0\n",
    "\n",
    "Environment:\n",
    "    OPENAI_API_KEY must be set.\n",
    "\"\"\"\n",
    "def list_openai_models() -> List[str]:\n",
    "    \"\"\"List all chat-completion compatible OpenAI models.\n",
    "\n",
    "    Returns:\n",
    "        List of chat-compatible model names.\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    try:\n",
    "        response = openai.models.list()\n",
    "        models = [model for model in response.data]\n",
    "        return models\n",
    "    except Exception as error:\n",
    "        print(f\"Failed to list models: {error}\")\n",
    "        return []\n",
    "\n",
    "def list_openai_chat_models() -> List[str]:\n",
    "    \"\"\"List all chat-completion compatible OpenAI models.\n",
    "\n",
    "    Returns:\n",
    "        List of chat-compatible model names.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.models.list()\n",
    "        chat_models = []\n",
    "        for model in response.data:\n",
    "            if \"chat\" in model.id.lower():\n",
    "                chat_models.append(model.id)\n",
    "        print(\"Available Chat Models:\")\n",
    "        return chat_models\n",
    "    except Exception as error:\n",
    "        print(f\"Failed to list models: {error}\")\n",
    "        return []\n",
    "\n",
    "def test_openai_connection(model) -> None:\n",
    "    \"\"\"Test OpenAI connection using the latest chat-compatible model.\"\"\"\n",
    "    # chat_models = list_chat_models()\n",
    "    if not model:\n",
    "        print(\"No chat-compatible models available.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nTesting model: {model}\\n\")\n",
    "\n",
    "    try:\n",
    "        llm = ChatOpenAI(model=model, temperature=1.0)\n",
    "        prompt_text = \"What is the capital of France? Respond with only the name of the city.\"\n",
    "        messages = [HumanMessage(content=prompt_text)]\n",
    "        response = llm.invoke(messages)\n",
    "\n",
    "        if response.content.strip().lower() == \"paris\":\n",
    "            print(\"CONNECTION SUCCESSFUL!\")\n",
    "            print(f\"Model ({llm.model_name}) responded correctly: {response.content}\")\n",
    "        else:\n",
    "            print(\"CONNECTION SUCCEEDED, but response was unexpected.\")\n",
    "            print(f\"Model Response: {response.content}\")\n",
    "\n",
    "    except Exception as error:\n",
    "        print(\"CONNECTION FAILED!\")\n",
    "        print(f\"Error Details: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb34b982-6cc7-4dce-aebc-76dbbe49aa5d",
   "metadata": {},
   "source": [
    "# Open AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f85df6c4-0ccb-4bd3-b2ab-814ec53acf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL: str = 'gpt-4.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64350552-f94c-4e2b-8b49-42448b0e588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_openai_key:str = os.path.expanduser('~/.openai/api_key')\n",
    "with open(path_to_openai_key, 'r', encoding='utf-8') as file:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16723d1e-1ad7-472c-bc88-e793aff11d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing model: gpt-4.1\n",
      "\n",
      "CONNECTION SUCCESSFUL!\n",
      "Model (gpt-4.1) responded correctly: Paris\n"
     ]
    }
   ],
   "source": [
    "test_openai_connection(MODEL)\n",
    "llm = init_chat_model(f\"openai:{MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea499b4-ec6c-4daf-aef8-129c80e935d7",
   "metadata": {},
   "source": [
    "# Chat Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894a7e53-7dfb-4a2f-8a79-e05edd9ea5e1",
   "metadata": {},
   "source": [
    "## State Machine Memory\n",
    "A ```StateGraph``` object defines the application as a graph. First step is to define its ```State``` object. The ```State``` includes the graph's schema and reducer functions that handle state updates. In our example, State is a schema with one key: messages. The reducer function is used to append new messages to the list instead of overwriting it. Keys without a reducer annotation will overwrite previous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b736285-5b7e-4c14-8c67-f24e915ba1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    \"\"\"Chat State (message history)\n",
    "    \"\"\"\n",
    "    messages: Annotated[\n",
    "        list, \n",
    "        add_messages\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c646aa-2f55-4682-8833-d65c88291fc3",
   "metadata": {},
   "source": [
    "## ChatBot Function as a Graph Node\n",
    "\n",
    "Add a ```chatbot``` function as a node to the graph. Nodes represent units of work and are typically regular functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daf68bf2-b787-4081-919b-d88771f2349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: State):\n",
    "    \"\"\"Chat Bot Function which is the basic pattern for all LangGraph node functions.\n",
    "    The add_messages function in the State instance will append the LLM's response messages.\n",
    "\n",
    "    Args:\n",
    "        state: chat history\n",
    "\n",
    "    \"\"\"\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "# The first argument is the unique node name and The second argument is \n",
    "# the function or object that will be called whenever he node is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b8a6208-92b2-4c06-a5c6-3b3ecf59ff12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x111418a30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Builder Pattern (GoF), not Factory pattern as you need to specify how to build step by step.\n",
    "app_graph_builder = StateGraph(State)\n",
    "\n",
    "# The first argument is the unique node name. The second argument is \n",
    "# the function or object that will be called whenever the node is used.\n",
    "app_graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d46b80f-63e1-4b68-82b1-7b93bbe8048c",
   "metadata": {},
   "source": [
    "## Add START entry point to the ```chatbot``` node in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5298915f-394a-4103-889c-d484fd0bf4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x111418a30>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_graph_builder.add_edge(START, \"chatbot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaab0ed-43fd-447d-a971-ca832565146c",
   "metadata": {},
   "source": [
    "## Add END exit point to from the ```chatbot``` node in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2846c73-00b2-4bc9-9cd4-4aa4696d8c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x111418a30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_graph_builder.add_edge(\"chatbot\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bdc249-224d-4c17-a1e7-e03ccba12a31",
   "metadata": {},
   "source": [
    "## Compile the Greaph\n",
    "\n",
    "Creates a ```CompiledGraph```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d95d7e0-88aa-4a85-8305-8027b5458308",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = app_graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1702abce-cfb8-40ef-be0d-0ba38080633b",
   "metadata": {},
   "source": [
    "### Verify the ChatBot as a Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90b6eba6-0e5b-456f-ba02-56c54b38699d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 400.\n",
      "\n",
      "To resolve this issue:\n",
      "1. Check your internet connection and try again\n",
      "2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n",
      "3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`\n"
     ]
    }
   ],
   "source": [
    "# Using https://mermaid.ink/\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60218a55-a9ea-4c32-b242-f5ee89b455f3",
   "metadata": {},
   "source": [
    "# Run Chat Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8731bf14-2132-422e-b92f-82f656c1309a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Does Kein and Avel depicts the human nature basis as a herd creature which needs acknowledgement without which cause anxiety envy jearous. Because this is so fundamental to human, the old testament put it as the 3rd chapter?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Your question is thought-provoking and ties together literary interpretation, psychology, and biblical analysis. Let's break it down and address each part:\n",
      "\n",
      "### 1. **Kain and Abel (Cain and Abel) & Human Nature**\n",
      "The story of Cain and Abel is found in Genesis 4, not chapter 3, but it's early in the Bible and thus foundational. The narrative is often interpreted as an exploration of human emotions—jealousy, envy, anger, and the need for recognition.\n",
      "\n",
      "- **Cain** offers a sacrifice that God does not accept; Abel's sacrifice is accepted. Cain then becomes envious and angry, ultimately killing Abel.\n",
      "- **Interpretation:** Many scholars, theologians, and even existentialist philosophers like Kierkegaard and social theorists like Freud or Girard have interpreted the story as illustrating deep aspects of human nature—especially our need for approval and the destructive effects when it is denied.\n",
      "\n",
      "### 2. **Herd Instinct/Recognition**\n",
      "You mention human nature as \"a herd creature which needs acknowledgement,\" leading to anxiety, envy, and jealousy when denied.\n",
      "\n",
      "- **Social Basis:** Modern psychology supports the idea that humans have evolved to seek recognition within their communities. Social status, inclusion, and acknowledgment are linked to our sense of security and self-worth.\n",
      "- **Old Testament Relevance:** Placing this story near the beginning of the Bible underlines the idea's primacy: human relations, rivalry, and the dire consequences of unacknowledged worth are foundational to the human experience.\n",
      "\n",
      "### 3. **Placement in the Bible**\n",
      "- Genesis 3 is about the Fall (Adam and Eve's expulsion from Eden). Genesis 4 is Cain and Abel.\n",
      "- Genesis thematically moves from the origin of humanity, the loss of innocence, to the breakdown of human relationships: **alienation from God** (the Fall), followed by **alienation from fellow humans** (Cain and Abel).\n",
      "\n",
      "### 4. **Does the Story Depict Herd-like Basis?**\n",
      "- **Not directly \"herd\" (as in social animal) in biblical language,** but the narrative certainly demonstrates the pain of exclusion and unacknowledged offerings.\n",
      "- Genesis 4 can be seen as reflecting the universal human need for recognition: When this need isn't met, it breeds destructive emotions.\n",
      "- Some interpreters, including those influenced by Girard’s \"mimetic theory,\" argue that rivalry and desire for the same acknowledgment is the root of violence—a herd phenomenon writ large.\n",
      "\n",
      "---\n",
      "\n",
      "## **Conclusion**\n",
      "Yes, in a sense, the story of Cain and Abel can be interpreted as an early biblical reflection on the fundamental human need for acknowledgment and the destructive consequences when this need is thwarted. This is not framed in the Old Testament as \"herd instinct\" per se, but the psychological foundation is similar: humans require social affirmation. When denied, it can give rise to destructive emotions such as anxiety, envy, and jealousy—fundamental themes addressed right at the beginning of the scriptural canon.\n",
      "\n",
      "**References for further reading:**\n",
      "- René Girard: *Violence and the Sacred*\n",
      "- Erich Fromm: *You Shall Be As Gods*\n",
      "- Jordan Peterson: Lectures on Genesis (YouTube)\n",
      "- Freud: The Future of an Illusion (discussion of religious origins and human psyche)\n",
      "\n",
      "If you'd like, I can expand on any of these points or recommend more resources!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Why it is in Genesis 4, not before, not after.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: That’s a rich and intriguing question! “Why it is in Genesis 4, not before, not after” refers to *something* in Genesis 4. To give a meaningful answer, I’ll need to clarify: what specifically do you mean by \"it\"? People asking this often mean the *story of Cain and Abel*, the *first murder*, or the idea of *offerings/sacrifice*.\n",
      "\n",
      "I'll address the question in a general sense, considering the primary events of Genesis 4: **the story of Cain and Abel**—the first murder, sibling rivalry, and the consequences of sin within human society.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. **Why is the Story of Cain and Abel in Genesis 4—Not Before?**\n",
      "\n",
      "**Genesis 1–3** establishes:\n",
      "- **Creation and cosmic order** (Genesis 1)\n",
      "- **Adam and Eve, humanity’s relation to God, and the first sin (the Fall)** (Genesis 2–3)\n",
      "\n",
      "***Genesis 4* introduces the next step**:\n",
      "- The movement **from the first human couple to the first family and the development of society**.\n",
      "- The escalation of sin: while Genesis 3 introduces sin (disobedience to God), Genesis 4 **shows its effects spreading—now culminating in violence and murder between humans.**\n",
      "- Thus, the **first murder and strife between humans cannot come “before” the creation of humans and the introduction of sin.**\n",
      "\n",
      "**Logical flow:**  \n",
      "Creation → First humans → Sin enters → *Effects of sin within family and society*\n",
      "\n",
      "### 2. **Why Not After Genesis 4?**\n",
      "\n",
      "Genesis 4 is **immediately after the fall** but precedes the Flood narrative (Genesis 6–9).\n",
      "- If the first murder and familial strife occurred later, it would disrupt the narrative sequence showing **sin’s progressive corruption of human relationships**.\n",
      "- The placement demonstrates that—**immediately after the initial break with God—the breakdown of human community follows swiftly.**\n",
      "\n",
      "### 3. **Theological/Literary Reason**\n",
      "\n",
      "The story’s **placement signals a pattern**:\n",
      "- Sin moves from **vertical** (between God and man in ch.3)  \n",
      "to **horizontal** (between humans in ch.4).\n",
      "- It shows how **alienation from God produces alienation from others**.\n",
      "- This foundational story *explains* why the world is broken, violent, and in need of redemption, setting up themes for the rest of Genesis and the Bible.\n",
      "\n",
      "### 4. **Summary Table**\n",
      "\n",
      "| Genesis Chapter | Key Event                      | Theological Purpose                            |\n",
      "|-----------------|-------------------------------|-----------------------------------------------|\n",
      "| 1               | Creation                      | God’s order                                   |\n",
      "| 2–3             | First humans, first sin       | Humanity’s relationship with God breaks       |\n",
      "| 4               | Cain & Abel, first murder     | Sin among humans, societal breakdown          |\n",
      "| 5–6             | Genealogies, growing wickedness| Prelude to judgment (the Flood)               |\n",
      "\n",
      "---\n",
      "\n",
      "**In Short:**  \n",
      "The story (whatever you mean by “it” in Genesis 4—most likely Cain and Abel) is there **because it narratively and theologically follows the fall: the first sin disables our relationship with God (ch.3); the second brings breakdown and violence between humans (ch.4). It couldn’t come before, because Cain and Abel weren’t born; not after, because it’s key to showing the rapid spread and deepening of sin.**\n",
      "\n",
      "If you meant something other than Cain and Abel, let me know and I’ll tailor the answer!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m         user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUser: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoodbye!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/venv/ml/lib/python3.10/site-packages/ipykernel/kernelbase.py:1251\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/ml/lib/python3.10/site-packages/ipykernel/kernelbase.py:1295\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        stream_graph_updates(user_input)\n",
    "    except Exception as e:\n",
    "        print(f\"failed due to {e}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b42d98-7ba1-4221-8b00-1628367fb2f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fecb01-0810-457b-b13e-a1adfa8f3e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
